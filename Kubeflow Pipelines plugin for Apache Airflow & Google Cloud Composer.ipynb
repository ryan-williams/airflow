{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Kubeflow Pipelines in Apache Airflow / Google Cloud Composer\n",
    "\n",
    "## In Cloud Composer\n",
    "\n",
    "### Create a Cloud Composer environment\n",
    "The full docs are [here](https://cloud.google.com/composer/docs/how-to/managing/creating), but here are some shortcuts:\n",
    "\n",
    "#### Enable the composer API\n",
    "```bash\n",
    "gcloud services enable composer.googleapis.com\n",
    "```\n",
    "\n",
    "#### Set a default zone for Composer to use\n",
    "```bash\n",
    "gcloud config set composer/location us-east1\n",
    "```\n",
    "\n",
    "#### Create a Cloud Composer cluster (\"environment\")\n",
    "```bash\n",
    "ENV=my-env\n",
    "gcloud composer environments create $ENV --python-version=3\n",
    "```\n",
    "\n",
    "This may take â‰ˆ20mins.\n",
    "\n",
    "### Give Cloud Composer's service account \"signBlob\" and \"IAP web user\" privileges\n",
    "Find Cloud Composer's service account:\n",
    "```bash\n",
    "SVCACCT=\"$(gcloud composer environments describe $ENV --format=\"get(config.nodeConfig.serviceAccount)\")\"\n",
    "```\n",
    "\n",
    "`signBlob` is necessary for uploading plugin and dag files to Google Cloud Storage (where Cloud Composer uses them):\n",
    "```bash\n",
    "gcloud projects add-iam-policy-binding \"${PROJECT}\" --member serviceAccount:\"${SVCACCT}\" --role roles/iam.serviceAccountTokenCreator\n",
    "```\n",
    "\n",
    "Allow Cloud Composer's service account to authenticate as an IAP-secured Web App User:\n",
    "```bash\n",
    "gcloud projects add-iam-policy-binding \"${PROJECT}\" --member serviceAccount:\"${SVCACCT}\" --role roles/iap.httpsResourceAccessor\n",
    "```\n",
    "See [IAP docs](https://cloud.google.com/iap/docs/managing-access) for more info.\n",
    "\n",
    "### Install KFP-Airflow plugin and example DAGs\n",
    "\n",
    "#### Plugin (containing KubeflowPipelinesOperator)\n",
    "```bash\n",
    "base=https://raw.githubusercontent.com/ryan-williams/airflow/kfp/airflow\n",
    "wget $base/contrib/operators/gcp_kubeflow_pipeline.py\n",
    "gcloud composer environments storage plugins import --environment $ENV --source gcp_kubeflow_pipeline.py\n",
    "```\n",
    "\n",
    "#### Example DAGs:\n",
    "```bash\n",
    "wget $base/example_dags/example_kubeflow_pipeline_dag.py\n",
    "gcloud composer environments storage dags import --environment $ENV --source example_kubeflow_pipeline_dag.py\n",
    "wget $base/example_dags/example_kubeflow_compile_pipeline_dag.py\n",
    "gcloud composer environments storage dags import --environment $ENV --source example_kubeflow_compile_pipeline_dag.py\n",
    "```\n",
    "\n",
    "### Configure Cloud Composer variables to point at an existing Kubeflow cluster\n",
    "\n",
    "```bash\n",
    "gcloud composer environments run $ENV variables -- --set KUBEFLOW_HOST \"https://${KFAPP}.endpoints.${PROJECT}.cloud.goog/pipeline\"\n",
    "gcloud composer environments run $ENV variables -- --set KUBEFLOW_OAUTH_CLIENT_ID \"$IAP_OAUTH_CLIENT_ID\"\n",
    "```\n",
    "\n",
    "See [the Kubeflow quickstart](https://www.kubeflow.org/docs/gke/deploy/deploy-cli/) for information about setting up a Kubeflow cluster and obtaining the placeholder values for the cluster location (`$KFAPP`, `$PROJECT`) as well as OAuth ID (`$IAP_OAUTH_CLIENT_ID`).\n",
    "\n",
    "### Navigate to Cloud Composer Web UI\n",
    "\n",
    "#### Find the web UI:\n",
    "```bash\n",
    "gcloud composer environments describe $ENV --format=\"get(config.airflowUri)\"\n",
    "```\n",
    "\n",
    "#### Open in browser:\n",
    "\n",
    "[![Cloud Composer / Airflow homepage](https://cl.ly/98b52dfdf552/Screen%20Shot%202019-06-24%20at%2012.30.02%20AM.png)](https://cl.ly/98b52dfdf552/Screen%20Shot%202019-06-24%20at%2012.30.02%20AM.png)\n",
    "\n",
    "Note the custom Kubeflow Pipelines example DAGs:\n",
    "- `example_kubeflow_compile_pipeline_operator` ([github](https://raw.githubusercontent.com/ryan-williams/airflow/kfp/airflow/example_dags/example_kubeflow_compile_pipeline_dag.py))\n",
    "- `example_kubeflow_pipeline_operator` ([github](https://raw.githubusercontent.com/ryan-williams/airflow/kfp/airflow/example_dags/example_kubeflow_pipeline_dag.py))\n",
    "\n",
    "### Trigger a simple Kubeflow Pipeline from Cloud Composer web UI\n",
    "The `example_kubeflow_pipeline_operator` example DAG runs a pipeline, [`coin.tar.gz`](https://storage.googleapis.com/ml-pipeline-playground/coin.tar.gz), with no additional inputs, so we can trigger a run from the web UI:\n",
    "\n",
    "[![Homepage showing \"Trigger Dag\" button](https://cl.ly/2076214826d6/[37736f19eb4baace4b90afc9b3239480]_Screen%20Shot%202019-06-24%20at%2012.31.02%20AM.png)](https://cl.ly/2076214826d6/[37736f19eb4baace4b90afc9b3239480]_Screen%20Shot%202019-06-24%20at%2012.31.02%20AM.png)\n",
    "\n",
    "Click \"Trigger Dag\" as shown, and a run will appear:\n",
    "\n",
    "[![Homepage showing a \"Running\" example DAG](https://cl.ly/d905250523ee/[ab427e0b67f4c49e8704b1c8e442f65a]_Screen%20Shot%202019-06-24%20at%2012.35.30%20AM.png)](https://cl.ly/d905250523ee/[ab427e0b67f4c49e8704b1c8e442f65a]_Screen%20Shot%202019-06-24%20at%2012.35.30%20AM.png)\n",
    "\n",
    "Refresh about a minute later, and you should see a \"success\" run in the \"Recent Tasks\" column:\n",
    "\n",
    "[![Homepage showing a recent success of an example DAG](https://cl.ly/7977176453c2/[ae37c5b84b66b21ac6db48393fe42b6c]_Screen%20Shot%202019-06-24%20at%2012.36.46%20AM.png)](https://cl.ly/7977176453c2/[ae37c5b84b66b21ac6db48393fe42b6c]_Screen%20Shot%202019-06-24%20at%2012.36.46%20AM.png)\n",
    "\n",
    "Click on that successful run, and you'll see a table with recent successful runs of this DAG:\n",
    "\n",
    "[![Recent successful example DAG runs, with \"Log Url\" link annotated](https://cl.ly/2f5313b5a7a1/[b16f74f02e87e330e78e66be10dd949f]_Screen%20Shot%202019-06-24%20at%2012.39.18%20AM.png)](https://cl.ly/2f5313b5a7a1/[b16f74f02e87e330e78e66be10dd949f]_Screen%20Shot%202019-06-24%20at%2012.39.18%20AM.png)\n",
    "\n",
    "Scroll all the way on the right and click on the \"Log Url\":\n",
    "\n",
    "[![Example DAG logs page, showing URL to Kubeflow Pipelines web UI for the run](https://cl.ly/3231486ea992/[ea9c6fdf0f95ca93ff91a5c5dde196b0]_Screen%20Shot%202019-06-24%20at%2012.48.03%20AM.png)](https://cl.ly/3231486ea992/[ea9c6fdf0f95ca93ff91a5c5dde196b0]_Screen%20Shot%202019-06-24%20at%2012.48.03%20AM.png)\n",
    "\n",
    "Some basic logs about the execution are here; in particular, the last line gives a link to the Kubeflow Pipelines web UI's \"run details\" page for the pipeline that was run as part of this Airflow DAG. Copying and navigating to that link:\n",
    "\n",
    "[![Kubeflow Pipelines web UI](https://cl.ly/647fccec88d1/Screen%20Shot%202019-06-24%20at%2012.51.20%20AM.png)](https://cl.ly/647fccec88d1/Screen%20Shot%202019-06-24%20at%2012.51.20%20AM.png)\n",
    "\n",
    "We see the Kubeflow Pipelines DAG, input/output, logs, etc.! ðŸŽ‰\n",
    "\n",
    "### Trigger an example DAG from the command-line\n",
    "\n",
    "The [`example_kubeflow_compile_pipeline_operator`](https://raw.githubusercontent.com/ryan-williams/airflow/kfp/airflow/example_dags/example_kubeflow_compile_pipeline_dag.py) DAG defines a Kubeflow Pipeline using the `@dsl.pipeline` annotation:\n",
    "\n",
    "```python\n",
    "@dsl.pipeline(\n",
    "    name='Sequential',\n",
    "    description='A pipeline with two sequential steps.'\n",
    ")\n",
    "def sequential_pipeline(filename='gs://ml-pipeline-playground/shakespeare1.txt'):\n",
    "    \"\"\"A simple example pipeline with two sequential steps.\"\"\"\n",
    "\n",
    "    op1 = dsl.ContainerOp(\n",
    "        name='getfilename',\n",
    "        image='library/bash:4.4.23',\n",
    "        command=['sh', '-c'],\n",
    "        arguments=['echo \"%s\" > /tmp/results.txt' % filename],\n",
    "        file_outputs={'newfile': '/tmp/results.txt'})\n",
    "    op2 = dsl.ContainerOp(\n",
    "        name='echo',\n",
    "        image='library/bash:4.4.23',\n",
    "        command=['sh', '-c'],\n",
    "        arguments=['echo \"%s\"' % op1.outputs['newfile']]\n",
    "    )\n",
    "```\n",
    "\n",
    "It then calls the ``, passing any DAG-run configuration values as parameters to the Kubeflow Pipeline:\n",
    "\n",
    "```python\n",
    "KubeflowPipelineOperator(\n",
    "    pipeline=sequential_pipeline,\n",
    "    params_fn=lambda params, conf: conf,\n",
    "    task_id='kubeflow-pipeline',\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "`params` are Airflow parameters, which can be set on the DAG or operator constructor calls. Here they're both empty, as we will pass in the pipeline's required input `filename` using the DAG-run configuration CLI flag:\n",
    "\n",
    "```bash\n",
    "gcloud composer environments run $ENV trigger_dag -- example_kubeflow_compile_pipeline_operator -c '{\"filename\":\"gs://ml-pipeline-playground/trainconfbin.json\"}'\n",
    "```\n",
    "\n",
    "Again, we see a \"running\" entry in the \"Recent Tasks\" column, this time for the `example_kubeflow_compile_pipeline_operator` row:\n",
    "\n",
    "[![Homepage showing the example_kubeflow_compile_pipeline_operator example DAG running](https://cl.ly/80e8f94bb860/[cc0d5d273ca578e25c1db41512ed4551]_Screen%20Shot%202019-06-24%20at%201.11.41%20AM.png)](https://cl.ly/80e8f94bb860/[cc0d5d273ca578e25c1db41512ed4551]_Screen%20Shot%202019-06-24%20at%201.11.41%20AM.png)\n",
    "\n",
    "Refreshing a few times, you should see it succeed. Clicking on the \"success\" counter, and then the \"Log Url\", you'll see the Kubeflow Pipelines link again:\n",
    "\n",
    "[![Kubeflow Pipelines web UI showing completed task and output](https://cl.ly/f0730407a4b8/[d557909bb82e8493de2bd77d22b4ffde]_Screen%20Shot%202019-06-24%20at%201.14.41%20AM.png)](https://cl.ly/f0730407a4b8/[d557909bb82e8493de2bd77d22b4ffde]_Screen%20Shot%202019-06-24%20at%201.14.41%20AM.png)\n",
    "\n",
    "The output path is indeed the one that we passed as input on the CLI, `gs://ml-pipeline-playground/trainconfbin.json` ðŸŽ‰.\n",
    "\n",
    "## In a local Airflow deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow-3.5.6",
   "language": "python",
   "name": "airflow-3.5.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
